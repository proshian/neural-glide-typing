{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "1. remove dropout from exported programs\n",
    "2. вероятно нужно сделать модули FullEncoder \n",
    "3. проверить, игнорируются поля модели, которые не используются в forward (например, есть ли разница между текущей инмплементацией `Decode` и имплементацией, где вся модель хранится как удинственное поле Decode) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union, Tuple\n",
    "import os\n",
    "import array\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.export import export, ExportedProgram, Dim\n",
    "from executorch.exir import EdgeProgramManager, to_edge, to_edge_transform_and_lower\n",
    "from executorch.exir.backend.backend_api import LoweredBackendModule, to_backend\n",
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "\n",
    "from model import MODEL_GETTERS_DICT\n",
    "from feature_extractors import get_val_transform\n",
    "from ns_tokenizers import CharLevelTokenizerv2, KeyboardTokenizerv1\n",
    "from word_generators_v2 import BeamGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = 'v3_weighted_and_traj_transformer_bigger'\n",
    "# CHECKPOINT_ROOT_PATH = '../../../checkpoints_for_executorch/my_weighted_features/'\n",
    "# CHECKPOINT_PATH = os.path.join(CHECKPOINT_ROOT_PATH, 'weighted_transformer_bigger-default--epoch=90-val_loss=0.444-val_word_level_accuracy=0.873.ckpt')  # 'weighted_transformer_bigger-default--epoch=115-val_loss=0.440-val_word_level_accuracy=0.879.ckpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATASET_ITEM_EXAMPLE = (\n",
    "    array.array('h', [567, 567, 507, 424, 380, 348, 337, 332, 330, 329, 327, 326, 326]),\n",
    "    array.array('h', [66, 66, 101, 161, 196, 230, 240, 245, 247, 249, 251, 251, 251]),\n",
    "    array.array('h', [0, 3, 24, 52, 75, 90, 106, 129, 145, 161, 177, 195, 209]),\n",
    "    'default',\n",
    "    'на')\n",
    "\n",
    "GRIDNAME_TO_GRID_PATH = '../data/data_separated_grid/gridname_to_grid.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND LINE ARGUMENTS EMULATION\n",
    "\n",
    "MODEL_NAME = 'v3_nearest_and_traj_transformer_bigger'\n",
    "CHECKPOINT_ROOT_PATH = '../../../checkpoints_for_executorch/my_nearest_features/'\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_ROOT_PATH, 'v3_nearest_and_traj_transformer_bigger-default--epoch=73-val_loss=0.444-val_word_level_accuracy=0.872.ckpt')\n",
    "TRANSFORM_NAME =  \"traj_feats_and_nearest_key\"\n",
    "\n",
    "DATA_ROOT = '../data/data_separated_grid'\n",
    "\n",
    "gridname_to_grid_path = os.path.join(DATA_ROOT, \"gridname_to_grid.json\")\n",
    "voc_path=os.path.join(DATA_ROOT, \"voc.txt\")\n",
    "char_tokenizer = CharLevelTokenizerv2(voc_path)\n",
    "kb_tokenizer = KeyboardTokenizerv1()\n",
    "\n",
    "USE_TIME = False\n",
    "USE_VELOCITY = True\n",
    "USE_ACCELERATION = True\n",
    "\n",
    "\n",
    "transform = get_val_transform(\n",
    "    gridname_to_grid_path=GRIDNAME_TO_GRID_PATH,\n",
    "    grid_names=['default'],\n",
    "    transform_name=TRANSFORM_NAME,\n",
    "    char_tokenizer=char_tokenizer,\n",
    "    uniform_noise_range=0,\n",
    "    include_time=USE_TIME,\n",
    "    include_velocities=USE_VELOCITY,\n",
    "    include_accelerations=USE_ACCELERATION,\n",
    "    dist_weights_func=None,  # Fill if weighted version is used\n",
    "    ds_paths_list=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(s: str, prefix: str) -> str:\n",
    "    if s.startswith(prefix):\n",
    "        s = s[len(prefix):]\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_state_dict_from_checkpoint(ckpt: dict) -> Dict[str, torch.Tensor]:\n",
    "    return {remove_prefix(k, 'model.'): v for k, v in ckpt['state_dict'].items()}\n",
    "\n",
    "\n",
    "def _prepare_encoder_input(encoder_in: Union[Tensor, Tuple[Tensor, Tensor]], \n",
    "                           device: str, batch_first: bool\n",
    "                           ) -> Tuple[Tensor, Tensor]:\n",
    "    is_tensor = None\n",
    "    if isinstance(encoder_in, Tensor):\n",
    "        is_tensor = True\n",
    "        encoder_in = [encoder_in]\n",
    "    else:\n",
    "        is_tensor = False\n",
    "\n",
    "    encoder_in = [el.unsqueeze(0) for el in encoder_in]\n",
    "    encoder_in = [el.to(device) for el in encoder_in]\n",
    "\n",
    "    if not batch_first:\n",
    "        encoder_in = [el.transpose(0, 1) for el in encoder_in]\n",
    "    return encoder_in[0] if is_tensor else encoder_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = get_state_dict_from_checkpoint(\n",
    "    torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=True))\n",
    "\n",
    "\n",
    "model = MODEL_GETTERS_DICT[MODEL_NAME]()\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(encoder_in, decoder_in), decoder_out_target = transform(RAW_DATASET_ITEM_EXAMPLE)\n",
    "encoder_in = _prepare_encoder_input(encoder_in, 'cpu', batch_first=False)\n",
    "if isinstance(encoder_in, list):\n",
    "    encoder_in = tuple(encoder_in)\n",
    "decoder_in = decoder_in.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = model.encode(\n",
    "    encoder_in, \n",
    "    None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = model.decode(decoder_in, encoded, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encode(torch.nn.Module):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.enc_in_emb_model = model.enc_in_emb_model\n",
    "        self.encoder = model.encoder\n",
    "\n",
    "    def forward(self, encoder_in):\n",
    "        x = self.enc_in_emb_model(encoder_in)\n",
    "        return self.encoder(x, src_key_padding_mask = None)\n",
    "\n",
    "def _get_casual_mask(sz: int, device='cpu'):\n",
    "    return torch.tril(\n",
    "        torch.ones((sz, sz), dtype=torch.bool, device=device),\n",
    "    )\n",
    "\n",
    "\n",
    "class Decode(torch.nn.Module):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.dec_in_emb_model = model.dec_in_emb_model\n",
    "        self.decoder = model.decoder\n",
    "        self._get_mask = model._get_mask\n",
    "        self.out = model.out\n",
    "        # MAX_WORD_LEN = 35\n",
    "        # causal_mask = self._get_casual_mask(MAX_WORD_LEN).to(device=self.model.device)\n",
    "        # self.register_buffer(\"causal_mask\", causal_mask, persistent=False)\n",
    "\n",
    "\n",
    "    def forward(self, decoder_in, x_encoded):\n",
    "        y = self.dec_in_emb_model(decoder_in)\n",
    "        tgt_mask = _get_casual_mask(y.size(0))  # = self.causal_mask[y.size(0):, y.size(0):]\n",
    "        dec_out = self.decoder(\n",
    "            y, x_encoded, tgt_mask=tgt_mask, \n",
    "            memory_key_padding_mask=None, \n",
    "            tgt_key_padding_mask=None,\n",
    "            tgt_is_causal=True)\n",
    "        return self.out(dec_out)\n",
    "    \n",
    "\n",
    "\n",
    "MAX_SWIPE_LEN = 299\n",
    "MAX_WORD_LEN = 35\n",
    "dim_swipe_seq = Dim(\"dim_swipe_seq\", min=1, max=MAX_SWIPE_LEN)\n",
    "dim_char_seq = Dim(\"dim_char_seq\", min=1, max=MAX_WORD_LEN)\n",
    "\n",
    "encoder_dynamic_shapes = {\"encoder_in\": ({0: dim_swipe_seq}, {0: dim_swipe_seq})}\n",
    "decoder_dynamic_shapes = {\n",
    "    \"x_encoded\": {0: dim_swipe_seq},\n",
    "    \"decoder_in\": {0: dim_char_seq}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "aten_encode: ExportedProgram = export(Encode(model).eval(), (encoder_in,))\n",
    "aten_decode: ExportedProgram = export(Decode(model).eval(), (decoder_in, encoded))\n",
    "\n",
    "# edge_program: EdgeProgramManager = to_edge(\n",
    "#     {\"encode\": aten_encode, \"decode\": aten_decode}\n",
    "# )\n",
    "\n",
    "\n",
    "# # edge_xnnpack: EdgeProgramManager = to_edge_transform_and_lower(\n",
    "# #     exported_program,\n",
    "# #     partitioner=[XnnpackPartitioner()],\n",
    "# # )\n",
    "\n",
    "# lowered_module: LoweredBackendModule = to_backend(\n",
    "#     graph_module = edge_program, partitioner=[XnnpackPartitioner()]\n",
    "# )\n",
    "\n",
    "\n",
    "# for method in edge_program.methods:\n",
    "#     print(f\"Edge Dialect graph of {method}\")\n",
    "#     print(edge_program.exported_program(method))\n",
    "\n",
    "\n",
    "edge_xnnpack: EdgeProgramManager = to_edge_transform_and_lower(\n",
    "    {\"encode\": aten_encode, \"decode\": aten_decode},\n",
    "    partitioner=[XnnpackPartitioner()],\n",
    ")\n",
    "\n",
    "\n",
    "exec_prog_xnnpack = edge_xnnpack.to_executorch()\n",
    "\n",
    "with open(\"xnnpack_my_nearest_feats.pte\", \"wb\") as file:\n",
    "    exec_prog_xnnpack.write_to_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(voc_path, 'r', encoding='utf-8') as f:\n",
    "    vocab = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_generator = BeamGenerator(model, char_tokenizer, 'cpu', vocab, max_token_id = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(encoder_in, decoder_in), decoder_out_target = transform(RAW_DATASET_ITEM_EXAMPLE)\n",
    "word_generator(encoder_in, max_steps_n=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executorch_examples_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
